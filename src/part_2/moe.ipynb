{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为Token序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和Tokens序列之间相互转化的函数，即可完成Tokenization部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单tokenizer， 由 字符表， 字符到token的 encoder()函数 和 token到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的tokenizer实现，比如openai 的tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataPath:str\n",
    "        ):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "        ):\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "        # # start token\n",
    "        # self.char2index[\"<START>\"] = 0\n",
    "        # self.index2char[0] = \"<START>\"\n",
    "        # # end token\n",
    "        # self.char2index[\"<END>\"] = 1\n",
    "        # self.index2char[1] = \"<END>\"\n",
    "        # # unknown token\n",
    "        # self.char2index[\"<UNK>\"] = 2\n",
    "        # self.index2char[2] = \"<UNK>\"\n",
    "        # scan the dataset\n",
    "        index = 0\n",
    "        for char in self.dataset:\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index] = char\n",
    "                index += 1\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence : str,\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3])\n",
    "\n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        for char in sentence:\n",
    "            if char not in self.char2index:\n",
    "                sentence = sentence.replace(char, \"\")\n",
    "        encoded = [self.char2index.get(char, 2) for char in sentence]\n",
    "        return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tokens : torch.Tensor,\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3])\n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        decoded = [self.index2char.get(index.item(), \"\") for index in tokens]\n",
    "        return \"\".join(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: 提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #         label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx + self.chunk_size]\n",
    "        label = self.encoded[idx + 1:idx + 1 + self.chunk_size]\n",
    "        if idx + 1 + self.chunk_size >= len(self.encoded):\n",
    "            label[-1] = self.encoded[0]\n",
    "        return chunk, label\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "\n",
    "def create_dataloader(filepath,\n",
    "                      tokenizer,\n",
    "                      chunk_size,\n",
    "                      batch_size,\n",
    "                      shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [int(len(dataset) * 0.8),\n",
    "         len(dataset) - int(len(dataset) * 0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = create_dataloader('input.txt',\n",
    "                                                     tokenizer,\n",
    "                                                     chunk_size=200,\n",
    "                                                     batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len: int, embed_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "\n",
    "        # TODO: init three matrix, to_q, to_k, to_v.\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "        # TODO: implement the attention mechanism\n",
    "\n",
    "        # Transform inputs into Query, Key, and Value vectors\n",
    "        query = self.to_q(inputs)\n",
    "        key = self.to_k(inputs)\n",
    "        value = self.to_v(inputs)\n",
    "\n",
    "        # Compute attention scores (affinity scores)\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))\n",
    "\n",
    "        # Apply the mask to the attention scores\n",
    "        mask = self.tril.to(attn_scores.device)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values to get context vectors\n",
    "        context_vecs = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到nn.ModuleList这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        #TODO: implement heads and projection\n",
    "        self.W_o = nn.Linear(n_heads * head_size, embed_size)\n",
    "        self.head_list = nn.ModuleList([HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)])\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "        # TODO:\n",
    "        for head in self.head_list:\n",
    "            head_output = head(inputs)\n",
    "            if 'outputs' not in locals():\n",
    "                outputs = head_output\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, head_output), dim=-1)\n",
    "        return self.W_o(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert即为标准Transformer中的FeedForward模块。\n",
    "\n",
    "在经过MultiHeadAttention 模块后，seq_len中的每一个Embedding都对应了前文信息的加权求和。在经过FeedForward模块时，模型对每一个位置的Embedding进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于MultiHeadAttention，我们在每一层训练多个FeedForward模块，对于不同位置的Embedding使用不同的FeedForward模块处理对应的信息。就好像每层有多个Expert,每个Expert都负责处理一类数据的深加工，因此我们称FeedForward为Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将x最后一维扩大至原先4倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size:int):\n",
    "        super().__init__()\n",
    "        #TODO: init two linear layer\n",
    "        self.linear1 = nn.Linear(embed_size, 4 * embed_size)\n",
    "        self.linear2 = nn.Linear(4 * embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 x embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        mid = torch.relu(self.linear1(inputs))\n",
    "        outputs = self.linear2(mid)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个Expert后，我们要设计一个选通网络决策每个Embedding要使用那个Expert计算\n",
    "\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16] \n",
    "\n",
    "即输入有batch_size=1个数据点，该数据有seq_len长度的context，即包含seq_len=8个Embedding，每个Embedding长度为embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个Embedding仅有 active_expert 个Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有seq_len=8的数据，如果每个Expert都参与计算每一个Embedding，那么一共需要计算 seq_len*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的active_experts个Expert，这要求我们对每一个Embedding计算最合适的active_experts个 Expert。\n",
    "\n",
    "对于单个Expert 的原版Transformer来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个Expert的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通0,2号Expert的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个Embedding ，我们使用神经网络对每个Expert打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前k大的分数的下标（即argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        ## TODO\n",
    "        ## embed_size : dimension of embedding\n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "\n",
    "        self.router = nn.Linear(embed_size, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        ## 完成这部分时，注意使用Softmax()对router_output做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        # score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "        # \\alpha = [1,0,1,0]\n",
    "        # ->\n",
    "        # mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "        # \\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "        # index = [1,0,1,0]\n",
    "        scores = self.router(inputs)\n",
    "        _, indices = torch.topk(scores, 2, dim=-1)\n",
    "        mask = torch.ones_like(scores, dtype=torch.bool)\n",
    "        mask = mask.scatter(-1, indices, False)\n",
    "        masked_scores = scores.masked_fill(mask, float('-inf'))\n",
    "        router_output = torch.softmax(masked_scores, -1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完Expert 和 TopkRouter后，我们可以定义SparseMoE模块。\n",
    "\n",
    "在前向过程中，对于inputs.shape = [Batch_size,seq_len,embed_size]第二维度seq_len个Embedding,我们先利用TopkRouter计算出选通专家序号indices以及专家权重router_output。\n",
    "\n",
    "我们将Embedding通过选通的Expert得出active_expert个新的Embedding，然后使用router_output的作为权重对新的Embedding加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size:int, num_experts:int, active_experts:int):\n",
    "        ## TODO\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.expert_list = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        router_output, indices = self.router(inputs)\n",
    "        final_output = torch.zeros_like(inputs)\n",
    "\n",
    "        flat_inputs = inputs.view(-1, inputs.size(-1))\n",
    "        flat_router_output = router_output.view(-1, router_output.size(-1))\n",
    "\n",
    "        for i, expert in enumerate(self.expert_list):\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_inputs[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                alpha = flat_router_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * alpha\n",
    "\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由一层层的block堆叠而成，其中每个block的结构从模型的结构图展开中可以看到，由LayerNorm，Masked multi head attention，(SparseMoE)FeedForward组成。\n",
    "\n",
    "对于一个表示句子的输入向量x，其首先会经过Layer Normalization层.\n",
    "Layer Normalization 层对于一个 句子个数x句子长度x单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中mean和var都是在最后两个维度上进行的，layernorm的实现同学们可以直接调用nn.LayerNorm\n",
    "经过layernorm层后，再经过Mask multi head attention层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层layernorm和feedforwad之后，就可以得到block块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(self, embed_size: int, n_heads: int, seq_len: int,\n",
    "                 num_experts: int, active_experts: int):\n",
    "        super().__init__()\n",
    "        # TODO: implement block structure\n",
    "        # MultiHeadAttention Layer\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads,\n",
    "                                            head_size=embed_size // n_heads,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embed_size=embed_size)\n",
    "\n",
    "        # Layer Normalization after Attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Sparse Mixture-of-Experts Layer\n",
    "        self.moe = SparseMoE(embed_size=embed_size,\n",
    "                             num_experts=num_experts,\n",
    "                             active_experts=active_experts)\n",
    "\n",
    "        # Layer Normalization after MoE\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        #TODO: forward with residual connection\n",
    "        # MultiHeadAttention\n",
    "        att_output = self.attention(inputs)  # MultiHeadAttention\n",
    "        att_output = self.norm1(att_output + inputs)  # Residual connection and layer normalization\n",
    "\n",
    "        # SparseMoE\n",
    "        moe_output = self.moe(att_output)  # Sparse Mixture-of-Experts\n",
    "        moe_output = self.norm2(moe_output + att_output)  # Residual connection and layer normalization\n",
    "\n",
    "        return moe_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    # Transformer decoder, consist of\n",
    "    # token embedding layer and position_embedding(position_embedding 可以理解为对位置编码，感兴趣的同学可以查阅原文，这里可以看为vocab_len = seq_len的Embedding)\n",
    "    # a stack of Transformer basic block\n",
    "    # a layernorm and output linear layer\n",
    "    def __init__(self, vocab_size: int, seq_len: int, embed_size: int,\n",
    "                 n_layers: int, n_heads: int, num_experts: int,\n",
    "                 active_experts: int):\n",
    "        # vocab_size is the number of word in vocabulary dict\n",
    "        # seq_len is the sequence length/sentence length\n",
    "        # embed_size is the embedding vector dimension\n",
    "        super().__init__()\n",
    "        # TODO:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Token Embedding Layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Positional Embedding\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "\n",
    "        # Stack of Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(embed_size=embed_size, n_heads=n_heads, seq_len=seq_len, num_experts=num_experts, active_experts=active_experts)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final LayerNorm\n",
    "        self.final_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Output Linear Layer\n",
    "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # labels: the (ground) true output\n",
    "        # TODO: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch_size, seq_len, )\n",
    "        _, T = inputs.shape\n",
    "        if T > self.seq_len:\n",
    "            raise ValueError(\"input sequence length exceeds the maximum length\")\n",
    "        # embedding:(batch_size, seq_len, embed_size)\n",
    "        token_embedding = self.token_embedding(inputs)\n",
    "        position_embedding = self.position_embedding(torch.arange(T, device=inputs.device))\n",
    "        x = token_embedding + position_embedding\n",
    "        # attens:(batch_size, seq_len, embed_size)\n",
    "        x = self.blocks(x)\n",
    "        # logits:(batch_size, seq_len, vocab_size)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # compute the loss\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = tokenizer.encode(inputs).unsqueeze(0)\n",
    "        device = next(self.parameters()).device\n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, :self.seq_len]\n",
    "        if inputs.size(1) < self.seq_len:\n",
    "            padding_str = \" \" * (self.seq_len - inputs.size(1))\n",
    "            padding = tokenizer.encode(padding_str).unsqueeze(0).to(inputs.device)\n",
    "            inputs = torch.cat([inputs, padding], dim=1)\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)\n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)\n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer \n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # TODO: implement the training process, and compute the training loss and validation loss\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # total_loss can not be nan\n",
    "        if total_loss != total_loss:\n",
    "            raise ValueError(\"Loss is nan\")\n",
    "\n",
    "    print(f'Epoch {epoch} Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    # TODO: 实现验证函数。与训练函数类似，但不需要计算梯度。\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        if total_loss != total_loss:\n",
    "            raise ValueError(\"Loss is nan\")\n",
    "\n",
    "    print(f'Epoch {epoch} Validation Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=64, batch_size=2024)\n",
    "model = SparseMoETransformer(vocab_size=len(tokenizer.char2index), seq_len=64, embed_size=256, n_layers=3, n_heads=8, num_experts=8, active_experts=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.6537009583038538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss: 1.2142863832078539\n",
      "Epoch 0 Train Loss: 1.6537009583038538, Valid Loss: 1.2142863832078539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.0597271069107141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.8110522208987055\n",
      "Epoch 1 Train Loss: 1.0597271069107141, Valid Loss: 0.8110522208987055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.6495344348910714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.42505942271636415\n",
      "Epoch 2 Train Loss: 0.6495344348910714, Valid Loss: 0.42505942271636415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.4362089855600647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.30451611704654524\n",
      "Epoch 3 Train Loss: 0.4362089855600647, Valid Loss: 0.30451611704654524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.34862232492083595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 0.27875448749946047\n",
      "Epoch 4 Train Loss: 0.34862232492083595, Valid Loss: 0.27875448749946047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def plot(train_losses, valid_losses):\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = len(train_losses)\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), train_losses, label='Train')\n",
    "    if valid_losses is not None:\n",
    "        plt.plot(range(epochs), valid_losses, label='Valid')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "# 训练模型\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        print(f'Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "    losses = {'train': train_losses, 'valid': valid_losses}\n",
    "    torch.save(losses, 'losses.log')\n",
    "\n",
    "#TODO: 用 matplotlib plot 训练过程中的 loss 变化\n",
    "\n",
    "run(model, train_dataloader, valid_dataloader, device, epochs=5)\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = torch.load('losses.log')\n",
    "plot(losses['train'], losses['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lance                                           thou, poor sails, is't and said\n",
      "Than a stranger him, and so much to give.\n",
      "\n",
      "GLOUCESTER:\n",
      "A greater gif\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
