{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为Token序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和Tokens序列之间相互转化的函数，即可完成Tokenization部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单tokenizer， 由 字符表， 字符到token的 encoder()函数 和 token到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的tokenizer实现，比如openai 的tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataPath:str\n",
    "        ):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "        ):\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "        # # start token\n",
    "        # self.char2index[\"<START>\"] = 0\n",
    "        # self.index2char[0] = \"<START>\"\n",
    "        # # end token\n",
    "        # self.char2index[\"<END>\"] = 1\n",
    "        # self.index2char[1] = \"<END>\"\n",
    "        # # unknown token\n",
    "        # self.char2index[\"<UNK>\"] = 2\n",
    "        # self.index2char[2] = \"<UNK>\"\n",
    "        # scan the dataset\n",
    "        index = 0\n",
    "        for char in self.dataset:\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index] = char\n",
    "                index += 1\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence : str,\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3])\n",
    "\n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        for char in sentence:\n",
    "            if char not in self.char2index:\n",
    "                sentence = sentence.replace(char, \"\")\n",
    "        encoded = [self.char2index.get(char, 2) for char in sentence]\n",
    "        return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tokens : torch.Tensor,\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3])\n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        decoded = [self.index2char.get(index.item(), \"\") for index in tokens]\n",
    "        return \"\".join(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: 提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #         label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx + self.chunk_size]\n",
    "        label = self.encoded[idx + 1:idx + 1 + self.chunk_size]\n",
    "        if idx + 1 + self.chunk_size >= len(self.encoded):\n",
    "            label[-1] = self.encoded[0]\n",
    "        return chunk, label\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "\n",
    "def create_dataloader(filepath,\n",
    "                      tokenizer,\n",
    "                      chunk_size,\n",
    "                      batch_size,\n",
    "                      shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset,\n",
    "        [int(len(dataset) * 0.8),\n",
    "         len(dataset) - int(len(dataset) * 0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader = create_dataloader('input.txt',\n",
    "                                                     tokenizer,\n",
    "                                                     chunk_size=200,\n",
    "                                                     batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len: int, embed_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "\n",
    "        # TODO: init three matrix, to_q, to_k, to_v.\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "        # TODO: implement the attention mechanism\n",
    "\n",
    "        # Transform inputs into Query, Key, and Value vectors\n",
    "        query = self.to_q(inputs)\n",
    "        key = self.to_k(inputs)\n",
    "        value = self.to_v(inputs)\n",
    "\n",
    "        # Compute attention scores (affinity scores)\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))\n",
    "\n",
    "        # Apply the mask to the attention scores\n",
    "        mask = self.tril.to(attn_scores.device)\n",
    "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values to get context vectors\n",
    "        context_vecs = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到nn.ModuleList这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        #TODO: implement heads and projection\n",
    "        self.W_o = nn.Linear(n_heads * head_size, embed_size)\n",
    "        self.head_list = nn.ModuleList([HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)])\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "        # TODO:\n",
    "        for head in self.head_list:\n",
    "            head_output = head(inputs)\n",
    "            if 'outputs' not in locals():\n",
    "                outputs = head_output\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, head_output), dim=-1)\n",
    "        return self.W_o(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert即为标准Transformer中的FeedForward模块。\n",
    "\n",
    "在经过MultiHeadAttention 模块后，seq_len中的每一个Embedding都对应了前文信息的加权求和。在经过FeedForward模块时，模型对每一个位置的Embedding进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于MultiHeadAttention，我们在每一层训练多个FeedForward模块，对于不同位置的Embedding使用不同的FeedForward模块处理对应的信息。就好像每层有多个Expert,每个Expert都负责处理一类数据的深加工，因此我们称FeedForward为Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将x最后一维扩大至原先4倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size:int):\n",
    "        super().__init__()\n",
    "        #TODO: init two linear layer\n",
    "        self.linear1 = nn.Linear(embed_size, 4 * embed_size)\n",
    "        self.linear2 = nn.Linear(4 * embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 x embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        mid = torch.relu(self.linear1(inputs))\n",
    "        outputs = self.linear2(mid)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个Expert后，我们要设计一个选通网络决策每个Embedding要使用那个Expert计算\n",
    "\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16] \n",
    "\n",
    "即输入有batch_size=1个数据点，该数据有seq_len长度的context，即包含seq_len=8个Embedding，每个Embedding长度为embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个Embedding仅有 active_expert 个Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有seq_len=8的数据，如果每个Expert都参与计算每一个Embedding，那么一共需要计算 seq_len*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的active_experts个Expert，这要求我们对每一个Embedding计算最合适的active_experts个 Expert。\n",
    "\n",
    "对于单个Expert 的原版Transformer来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个Expert的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通0,2号Expert的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个Embedding ，我们使用神经网络对每个Expert打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前k大的分数的下标（即argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        ## TODO\n",
    "        ## embed_size : dimension of embedding\n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "\n",
    "        self.router = nn.Linear(embed_size, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        ## 完成这部分时，注意使用Softmax()对router_output做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        # score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "        # \\alpha = [1,0,1,0]\n",
    "        # ->\n",
    "        # mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "        # \\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "        # index = [1,0,1,0]\n",
    "        scores = self.router(inputs)\n",
    "        _, indices = torch.topk(scores, 2, dim=-1)\n",
    "        mask = torch.ones_like(scores, dtype=torch.bool)\n",
    "        mask = mask.scatter(-1, indices, False)\n",
    "        masked_scores = scores.masked_fill(mask, float('-inf'))\n",
    "        router_output = torch.softmax(masked_scores, -1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完Expert 和 TopkRouter后，我们可以定义SparseMoE模块。\n",
    "\n",
    "在前向过程中，对于inputs.shape = [Batch_size,seq_len,embed_size]第二维度seq_len个Embedding,我们先利用TopkRouter计算出选通专家序号indices以及专家权重router_output。\n",
    "\n",
    "我们将Embedding通过选通的Expert得出active_expert个新的Embedding，然后使用router_output的作为权重对新的Embedding加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size:int, num_experts:int, active_experts:int):\n",
    "        ## TODO\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.expert_list = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        router_output, indices = self.router(inputs)\n",
    "        final_output = torch.zeros_like(inputs)\n",
    "\n",
    "        flat_inputs = inputs.view(-1, inputs.size(-1))\n",
    "        flat_router_output = router_output.view(-1, router_output.size(-1))\n",
    "\n",
    "        for i, expert in enumerate(self.expert_list):\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_inputs[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                alpha = flat_router_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * alpha\n",
    "\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由一层层的block堆叠而成，其中每个block的结构从模型的结构图展开中可以看到，由LayerNorm，Masked multi head attention，(SparseMoE)FeedForward组成。\n",
    "\n",
    "对于一个表示句子的输入向量x，其首先会经过Layer Normalization层.\n",
    "Layer Normalization 层对于一个 句子个数x句子长度x单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中mean和var都是在最后两个维度上进行的，layernorm的实现同学们可以直接调用nn.LayerNorm\n",
    "经过layernorm层后，再经过Mask multi head attention层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层layernorm和feedforwad之后，就可以得到block块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(self, embed_size: int, n_heads: int, seq_len: int,\n",
    "                 num_experts: int, active_experts: int):\n",
    "        super().__init__()\n",
    "        # TODO: implement block structure\n",
    "        # MultiHeadAttention Layer\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads,\n",
    "                                            head_size=embed_size // n_heads,\n",
    "                                            seq_len=seq_len,\n",
    "                                            embed_size=embed_size)\n",
    "\n",
    "        # Layer Normalization after Attention\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Sparse Mixture-of-Experts Layer\n",
    "        self.moe = SparseMoE(embed_size=embed_size,\n",
    "                             num_experts=num_experts,\n",
    "                             active_experts=active_experts)\n",
    "\n",
    "        # Layer Normalization after MoE\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        #TODO: forward with residual connection\n",
    "        # MultiHeadAttention\n",
    "        att_output = self.attention(inputs)  # MultiHeadAttention\n",
    "        att_output = self.norm1(att_output + inputs)  # Residual connection and layer normalization\n",
    "\n",
    "        # SparseMoE\n",
    "        moe_output = self.moe(att_output)  # Sparse Mixture-of-Experts\n",
    "        moe_output = self.norm2(moe_output + att_output)  # Residual connection and layer normalization\n",
    "\n",
    "        return moe_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    # Transformer decoder, consist of\n",
    "    # token embedding layer and position_embedding(position_embedding 可以理解为对位置编码，感兴趣的同学可以查阅原文，这里可以看为vocab_len = seq_len的Embedding)\n",
    "    # a stack of Transformer basic block\n",
    "    # a layernorm and output linear layer\n",
    "    def __init__(self, vocab_size: int, seq_len: int, embed_size: int,\n",
    "                 n_layers: int, n_heads: int, num_experts: int,\n",
    "                 active_experts: int):\n",
    "        # vocab_size is the number of word in vocabulary dict\n",
    "        # seq_len is the sequence length/sentence length\n",
    "        # embed_size is the embedding vector dimension\n",
    "        super().__init__()\n",
    "        # TODO:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Token Embedding Layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Positional Embedding\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "\n",
    "        # Stack of Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(embed_size=embed_size, n_heads=n_heads, seq_len=seq_len, num_experts=num_experts, active_experts=active_experts)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final LayerNorm\n",
    "        self.final_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Output Linear Layer\n",
    "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # labels: the (ground) true output\n",
    "        # TODO: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch_size, seq_len, )\n",
    "        _, T = inputs.shape\n",
    "        if T > self.seq_len:\n",
    "            raise ValueError(\"input sequence length exceeds the maximum length\")\n",
    "        # embedding:(batch_size, seq_len, embed_size)\n",
    "        token_embedding = self.token_embedding(inputs)\n",
    "        position_embedding = self.position_embedding(torch.arange(T, device=inputs.device))\n",
    "        x = token_embedding + position_embedding\n",
    "        # attens:(batch_size, seq_len, embed_size)\n",
    "        x = self.blocks(x)\n",
    "        # logits:(batch_size, seq_len, vocab_size)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # compute the loss\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = tokenizer.encode(inputs).unsqueeze(0)\n",
    "        device = next(self.parameters()).device\n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, :self.seq_len]\n",
    "        if inputs.size(1) < self.seq_len:\n",
    "            padding_str = \" \" * (self.seq_len - inputs.size(1))\n",
    "            padding = tokenizer.encode(padding_str).unsqueeze(0).to(inputs.device)\n",
    "            inputs = torch.cat([inputs, padding], dim=1)\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)\n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)\n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer \n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # TODO: implement the training process, and compute the training loss and validation loss\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # total_loss can not be nan\n",
    "        if total_loss != total_loss:\n",
    "            raise ValueError(\"Loss is nan\")\n",
    "\n",
    "    print(f'Epoch {epoch} Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    # TODO: 实现验证函数。与训练函数类似，但不需要计算梯度。\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        if total_loss != total_loss:\n",
    "            raise ValueError(\"Loss is nan\")\n",
    "\n",
    "    print(f'Epoch {epoch} Validation Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=64, batch_size=2024)\n",
    "model = SparseMoETransformer(vocab_size=len(tokenizer.char2index), seq_len=64, embed_size=256, n_layers=3, n_heads=8, num_experts=8, active_experts=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.6476956278829078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss: 1.210537981342625\n",
      "Epoch 0 Train Loss: 1.6476956278829078, Valid Loss: 1.210537981342625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:01<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.0793107035749354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 0.8487004900837803\n",
      "Epoch 1 Train Loss: 1.0793107035749354, Valid Loss: 0.8487004900837803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:02<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.7142231398834393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 0.4828995595107207\n",
      "Epoch 2 Train Loss: 0.7142231398834393, Valid Loss: 0.4828995595107207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.4868846118044691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 0.324985440518405\n",
      "Epoch 3 Train Loss: 0.4868846118044691, Valid Loss: 0.324985440518405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [02:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.3788902074301324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 0.2891933952902888\n",
      "Epoch 4 Train Loss: 0.3788902074301324, Valid Loss: 0.2891933952902888\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot(train_losses, valid_losses):\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = len(train_losses)\n",
    "    plt.plot(range(epochs), train_losses, label='Train')\n",
    "    if valid_losses is not None:\n",
    "        plt.plot(range(epochs), valid_losses, label='Valid')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "# 训练模型\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        print(f'Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "    plot(train_losses, valid_losses)\n",
    "\n",
    "#TODO: 用 matplotlib plot 训练过程中的 loss 变化\n",
    "\n",
    "run(model, train_dataloader, valid_dataloader, device, epochs=5)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lance                                           order?\n",
      "\n",
      "BUCKINGHAM:\n",
      "And, in good time, here the lieutenant comes.\n",
      "Master lieutenant, pray you, by yo\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
